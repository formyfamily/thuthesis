\chapter{引言}
\label{cha:introduction}

\section{研究背景}

计算机图形学是一门涉及计算几何、数值分析以及物理学等多方知识的综合学科，其主要的关注重点是如何在计算机中生成并展示图像、影视等图形素材。
在信息技术高度发展的今天，计算机极大程度地解放了人们的生产力，使得许多原本繁杂的工作变得迅速而方便。
图形学作为计算机科学技术中的重要一员，则为人类提供了更加便捷的人机交互过程、功能强大的三维建模技术、以及高度逼真的图像、影视素材等诸多福利。

在图形学当中，真实感渲染技术（以下简称为渲染技术）始终是最受关注的热门课题之一。
这类技术通常以包含物理信息（如物体几何信息、材质信息、光照信息、时间信息）的场景，以及用于观测的摄像机参数作为输入，
以生成与真实世界近似的仿真图像（或影像）作为最终的目标。1968年，由Arther Appel提出的光线投射(Ray Casting)算法\cite{???},被认为是该领域的开创技术。
经过50余年的发展，目前的渲染技术已经可以生成与真实世界非常近似的结果，并已经做到了高度产业化，在游戏、影视等大量产业中都起着主导作用。
尽管如此，直至今日该领域的学者与工程师们仍面临着两个主要问题需要解决：其一是如何更好地支持复杂的物理模型或过程，如流体粒子、弹性形变等；
其二则是如何提高渲染效率，在更短的时间内得到更加真实的效果。本文中所探讨的正是其中的第二个问题。

按照渲染流程所需的时间划分，当今的渲染技术可以大致分为两种：实时渲染和非实时渲染。
实时渲染技术往往以降低结果的真实性作为代价，以求达到高于人眼观察频率的绘制速度，多数被用于游戏、建模、医疗等行业之中；
而非实时渲染技术的主要目的则是得到最具真实性、观赏性的结果，在影视、设计等领域应用较多。
在实时渲染中，基于光栅化的渲染技术使用最为普遍，该类技术往往通过一个确定的渲染管线（Pipeline），
在屏幕空间中并行计算出各个像素点的输出值；基于路径追踪的渲染技术，则以其效果较好但效率较低的特点，
更多地被使用到非实时渲染中，这类技术会通过向场景中投射并追踪光线，从而计算出屏幕各个像素点在场景中实际观测到的光照强度。

除去算法之外，渲染技术对于计算机硬件也有着独特的需求。早期的渲染技术普遍都是通过
单核的中央处理器（CPU）完成，渲染计算高度并行化的特点在其中并没有被得到很好的利用。
为了提高效率，专用于图形计算的图形处理器应运而生，它以并行性高，运算规模大的特点，
在图形处理上获得了数倍于前者的速度。然而，在追求并行性的同时，GPU对于渲染程序有着严格的限制，因此在相当长时间内仅仅被用于基于光栅化的渲染之中。
直到近几年来，伴随着各类基于GPU的通用并行编程语言（例如CUDA），以及GPU本身算力、架构的发展，
利用GPU实现的光线追踪渲染也逐渐由不可能转为可能，出现在世人面前。

\section{图形处理器}
图形处理器（或称作显卡）最早的雏形可以追溯至在上世纪70年代，但直至90年代，这一概念才逐渐被人们广为所知。
1999年NVIDIA公司发布产品GeForce 256时，将其称之为“世界上第一款显卡”\cite{firstGPU}，并引起了强烈的市场反响。
之后的二十年内，GPU进入高速发展期，目前主要由NVIDIA、AMD以及Intel三家厂商主导。

按照目前的一般定义来说，图形处理器实际是一种单指令流多数据流(SIMD)的专用处理器，由流处理单元、寄存器、缓存等多个部分组成，
主要的针对目标渲染过程中几何变换，求解光强以及三角面片处理中的计算。
对于图形处理器而言，高效率和并行化显得尤为关键，因此它对运行过程中的缓存分配、任务调度提出了很高的要求。
然而，考虑到设计复杂度，上述的这些处理通常都会交由软件完成，这直接导致了编写GPU程序极为困难的情况。
为了让用户跳过与硬件之间的复杂交互，OpenGL库得以诞生，将此类过程封装成了一个相对而言更为简单的API。

在早期OpenGL中，图形渲染的过程都会通过固定的管线(Pipeline)完成，
用户只能通过有限的API修改管线中的内容（诸如参数、调用方式等）以及渲染中所提供的数据（一般称为Buffer）
来获得不同的渲染结果。这样的编程模式所支持的功能实际十分有限。为了解决这一问题，OpenGL 2.0引入了可编程管线，
用户可以通过编写名为Shader的程序，修改管线中部分模块（如顶点着色器、片元着色器等）的函数过程。
这样的编程模式一直保留至今，是目前基于光栅化的渲染技术的主流实现方式。

另一方面，GPU的设计者们意识到这类硬件在并行计算上的强劲实力，因此开始将目标转向渲染之外的计算之中。
通用图形处理器，即General Purpose GPU（GPGPU），正是将GPU从图形渲染扩展至通用并行计算的结果。
这类处理器除了在架构上有所改进外，更为重要的是为用户提供了一种新的GPU编程模式。这种模式不再
受限于传统的管线结构，而为用户提供了充分的权利直接编写并行计算程序。CUDA、OpenCL是这类编程模式中最为知名的代表。
它们的出现大大促进了诸如机器学习、科学计算、图像处理等诸多学科的发展。最终也反过来导致了渲染技术的进一步发展。

\section{渲染技术与机器学习的交叉}

正如上一节所示，渲染技术的不断发展间接促成了GPGPU的诞生，从而使得机器学习拥有了更加强大的算力基础。
对于诸多以线性代数、概率论为基础的机器学习算法而言，训练模型的过程往往在不同的数据以及被训练参数之间具有相互独立性，
因此为算法的并行化提供了可行性。在各类机器学习的子领域中，深度学习首当其冲，成为受到影响最为显著的一门学科。
深度学习的基础理论，如bp算法、CNN模型等，早在数十年前便已提出，却由于算力不足的原因遭遇发展瓶颈，
直至近年在GPU的帮助下，各类更为先进的网络模型诸如AlexNet、ResNet、U-Net以至GAN、VAE等相继落地成型，
其发展迅速对整个社会都产生了轰动性的影响。

除去硬件带来的催化作用外，渲染算法本身也为机器学习提供了其他支持。
利用渲染算法生成的图片，可以为计算机视觉中的模型提供人工生成的训练数据，
这些数据往往具有精确的场景信息（如几何信息、语义信息），但在输入的图片上往往会带来偏差（相对于现实生活图片）。

另一方面，机器学习的发展，反过来为提供更好的渲染效果带来了帮助。
例如，诸多机器学习模型为渲染生成的图片提供了各式各样的后处理，如去噪、抗锯齿、提高分辨率等操作；
一些研究关注通过神经网络近似难以渲染的模型，如多次散射模型等等。
考虑到以上算法的不断壮大，最近提出的RTX系列显卡中更是直接搭载了用于进行神经网络计算的模块Tensor Core\cite{RTX}，
对支持实时光线追踪渲染技术提供了很大帮助。

关于渲染技术与机器学习两者交叉而产生的一些算法，在下一章中会有更加详细的介绍。

\section{研究价值与意义}

本文的出发点正是受到GPU光线追踪渲染技术的启发，
通过已有的GPU路径追踪库OpTiX设计出一套高效率，高复用，可扩展性强的交互式渲染架构，
从而对GPU下的光线追踪渲染技术进行测试，探讨GPU在光线追踪渲染中的实际效果和未来发展趋势。

事实上，目前诸多主流的三维渲染软件如AutoDesk 3dsMax、Blender等都已经配备了GPU渲染器，
然而这些渲染器在大多由于商用原因或缺少公开的源代码，或在设计上过于复杂，对于学术研究而言缺乏实用性。
另一方面，近来许多利用渲染技术生成机器学习数据的工作中，也使用到了GPU进行光线追踪渲染，
这类渲染器则大多只是为完成具体的渲染任务而设计，缺乏一定的扩展性和复用性。
相比之下，本研究希望探究的正是一套介于两者之间的解决方案，其既具有良好的封装，能够保持充分的可扩展性，
又在设计上做到足够简化，以方便使用者进行维护。

从长远角度来说，该项研究的意义可以认为主要有以下三点：
其一是可以为本系实验室中与计算机视觉相关的研究提供快速、方便的数据生成支持；
其二是可以为目前以及将来的渲染研究提供实验平台；
最后则是为本系的图形学教学提供教学资源。

本文接下来各章的内容安排如下所示：
第2章将会对目前已有的工作进行综合介绍；
第3章会介绍基于OpTiX而设计的渲染器总体架构；
第4章将介绍在该渲染器中一些主流渲染算法的移植；
第5章主要介绍在渲染器下对VRay材质的模拟；
第6章说明用于测试的实验方法及实验结果；
最后一章对研究进行总结。






