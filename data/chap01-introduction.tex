\chapter{引言}
\label{cha:introduction}

\section{研究背景}

计算机图形学是一门涉及计算几何、数值分析以及物理学等多方知识的综合学科，其主要的关注重点是如何在计算机中生成并展示图像、影视等图形素材。
在信息技术高度发展的今天，计算机极大程度地解放了人们的生产力，使得许多原本繁杂的工作变得迅速而方便。
图形学作为计算机科学技术中的重要一员，则为人类提供了更加便捷的人机交互过程、功能强大的三维建模技术、高度逼真的图像、影视素材等诸多福利。

在图形学当中，真实感渲染技术（以下简称为渲染技术）始终是最受关注的热门课题之一。
这类技术通常以包含物理信息（如物体几何信息、材质信息、光照信息、时间信息）的场景，以及用于观测的相机参数作为输入，
而以生成与真实世界近似的图像（或影像）作为目标。1968年，由Arther Appel提出的光线投射(Ray Casting)算法\cite{Appel:1968:TSM:1468075.1468082},被认为是该领域的开创技术。
经过50余年的发展，目前的渲染技术已经发展到可以生成与真实世界非常近似的结果，并做到了高度产业化，在游戏、影视等大量产业中都起到了主导作用。
尽管如此，该领域的学者与工程师们依然在尝试从两个角度进一步改进这项技术：其一是如何更好地支持复杂的物理现象，如流体粒子运动、光谱散射等等；
其二则是如何提高渲染效率，在更短的时间内得到更加真实的效果。

按照渲染流程所需的时间划分，当今的渲染技术可以大致分为两种：实时渲染和非实时渲染。
实时渲染技术往往以降低结果的真实性作为代价，以求达到高于人眼观察频率的绘制速度，多数被用于游戏、建模、医疗等行业之中；
而非实时渲染技术则普遍追求最具真实性、观赏性的结果，在影视、设计等领域应用较多。
在实时渲染中，基于光栅化的渲染技术使用最为普遍，该类技术往往通过一个确定的渲染管线（Pipeline），
在屏幕空间中并行计算出各个像素点的输出值；而基于路径追踪的渲染技术，则以其效果较好但效率较低的特点，
更多地被使用到非实时渲染中，这类技术会通过向场景中投射并追踪光线，从而计算出屏幕各个像素点在场景中实际观测到的光照亮度。

除去算法之外，渲染技术对于计算机硬件也有着独特的需求。早期的渲染技术普遍都是通过
单核的中央处理器（CPU）完成，渲染计算高度并行化的特点在其中并没有被得到很好的利用。
为了提高效率，专用于图形计算的图形处理器应运而生，它以并行性高，运算规模大的特点，
在图形处理上获得了数倍于前者的速度。然而，在追求并行性的同时，GPU对于渲染程序有着严格的限制，因此在相当长时间内仅仅被用于基于光栅化的渲染之中。
直到近几年来，伴随着各类基于GPU的通用并行编程语言（例如CUDA），以及GPU本身算力、架构的发展，
利用GPU实现的光线追踪渲染才逐渐由不可能转为可能，开始出现在世人面前。

\section{图形处理器}
图形处理器（或称作显卡）最早的雏形可以追溯至在上世纪70年代，但直至90年代，这一概念才逐渐被人们广为所知。
1999年Nvidia公司发布产品GeForce 256时，将其称之为“世界上第一款显卡”\cite{firstGPU}，并引起了强烈的市场反响。
之后的二十年内，GPU进入高速发展期，目前的研发工作主要由Nvidia、AMD以及Intel三家厂商主导。

按照一般定义来说，图形处理器实际是一种单指令流多数据流(SIMD)的专用处理器，由流处理单元、寄存器、缓存等多个部分组成，
主要擅长于完成渲染过程中几何变换，光强计算以及三角面片处理等一系列操作。
对于图形处理器而言，高效率和并行化显得尤为关键，因此它对运行过程中的缓存分配、任务调度提出了很高的要求。
然而，考虑到设计复杂度，上述的这些处理通常都会交由软件完成，这直接导致了编写GPU程序极为困难的情况。
为了让用户跳过与硬件之间的直接交互，OpenGL库得以诞生，
它将调用GPU进行运算的过程封装成了一个相对而言更为简单的API库。

在早期OpenGL中，图形渲染的过程都会通过固定的管线(Pipeline)完成，
用户只能通过有限的接口函数修改管线中的指定内容（诸如参数、调用方式等）以及渲染中要使用的数据
来获得不同的渲染结果。这样的编程模式所支持的功能实际十分有限。
为了解决这一问题，OpenGL在2.0版本里引入了可编程管线，
用户可以通过编写名为Shader的程序，修改管线中部分模块（如顶点着色器、片元着色器等）的函数计算过程。
这样的编程模式一直保留至今，是目前绝大多数基于光栅化的渲染技术的主要实现方式。

另一方面，意识到GPU在并行计算上的强劲实力，它的设计者开始将目标扩展到渲染之外的计算任务之中。
通用图形处理器，即General Purpose GPU（GPGPU），正是将GPU的用途从图形渲染扩展至通用并行计算的结果。
这类处理器除了在架构上有所改进外，更为重要的是为用户提供了一种新的GPU编程模式。用户不再
受限于传统的管线结构，而是拥有了充分的自由度直接编写并行计算程序。CUDA、OpenCL都属于这类编程语言中最为知名的代表之一。
它们的出现大大促进了诸如机器学习、科学计算、图像处理等诸多学科的发展，最终也反过来引发了渲染技术的进一步提升。

\section{渲染技术与机器学习的交叉}

正如上一节所示，渲染技术的不断发展间接促成了GPGPU的诞生，从而为机器学习提供了更加强大的算力基础。
对于一般的机器学习算法而言，训练模型的过程往往可以针对不同的数据和被训练参数拆分成多个相互独立的并行任务，
因此为算法整体的并行化提供了可行性。在机器学习的各个子领域中，深度学习首当其冲，成为受到影响最为显著的一门学科。
深度学习的基础理论，如bp算法、CNN模型等，早在数十年前便已提出，却由于算力不足的原因曾一度遭遇发展瓶颈，
直至近年在GPU的帮助下，各类更为先进的网络模型诸如AlexNet\cite{Krizhevsky:2012:ICD:2999134.2999257}、ResNet\cite{He2016DeepRL}以至GAN\cite{goodfellow2014generative}、VAE\cite{kingma2013auto}等相继落地成型，
其发展迅速对整个社会都产生了轰动性的影响。

除去硬件带来的催化作用外，渲染算法本身也为机器学习，尤其是计算机视觉方面的算法提供了一些支持。
利用渲染算法生成的图片，可以被用做成计算机视觉算法的训练数据，
这些数据通常具有精确的场景信息（如几何信息、语义信息），但在输入图像上含有一定偏差（相对于现实世界的照片）。

机器学习的发展为提供更好的渲染效果同样有所帮助。
例如，诸多机器学习模型为渲染生成的图片提供了各式各样的后处理，如去噪、抗锯齿、提高分辨率等操作；
一些研究关注通过神经网络近似难以渲染的模型，如多次散射模型等等。
考虑到以上算法的不断壮大，Nvidia公司在其最近提出的系列显卡中更是直接搭载了专用于神经网络计算的模块Tensor Core，
以支持其声称的实时光线追踪渲染功能。

关于渲染技术与机器学习两者的交叉算法，在下一章中会有更加详细的介绍。

\section{研究价值与意义}

本文的出发点正是受到GPU光线追踪渲染技术的启发，
通过已有的GPU路径追踪库OpTiX设计出一套高效率，高复用，可扩展性强的交互式渲染架构，
从而探讨GPU在光线追踪渲染技术中的实际运用效果和未来发展趋势。

事实上，目前诸多主流的三维渲染软件如AutoDesk 3dsMax、Blender等都已经配备了GPU渲染器，
然而这些渲染器在大多由于商用原因或缺少公开的源代码，或在设计上过于复杂，对于学术研究而言缺乏实用性。
另一方面，近年来在许多通过渲染生成机器学习数据的工作中，也有一些GPU光线追踪渲染器的实现，
这类渲染器则大多只是为完成具体的渲染任务而设计，缺乏一定的扩展性和复用性。
相比之下，本研究希望探究的正是一套介于两者之间的解决方案，其既具有良好的封装结构，能够保证代码的充分可扩展，
又在设计上做到足够简化，以方便人们对其进行维护和使用。

从长远角度来说，该项研究的意义可以认为主要有以下三点：
其一是可以为本系实验室中与计算机视觉相关的研究提供快速、方便的数据生成支持；
其二是可以为目前以及将来的渲染研究提供实验平台；
最后则是为本系的图形学教学提供资源。

以下是本文接下来各章的大致内容：
第2章将会对目前已有的相关工作进行综合介绍；
第3章会介绍本人设计的渲染器总体架构；
第4章将介绍在该渲染器中一些主流渲染算法的移植情况；
第5章主要介绍在渲染器下对VRay材质的模拟；
第6章进行实验测试；
最后一章对研究进行总结。






